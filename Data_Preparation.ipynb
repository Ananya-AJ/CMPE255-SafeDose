{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananya-AJ/CMPE255-SafeDose/blob/main/Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**This colab provides a step-by-step data preparation pipeline for the project \n",
        "'Safe Dose'. The data preparation steps are functionalized and can be called sequentially to obtain the final processed dataset.**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KuFKC91CUBo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "!pip install category_encoders"
      ],
      "metadata": {
        "id": "WqBEN8vHiI5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oYNesbFVeop"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import category_encoders as ce\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFDeSOGuVlqd",
        "outputId": "374748b0-56de-43f3-ba01-414706635786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import raw data and segregate data into demographic information, case related information and drug related information for easier processing."
      ],
      "metadata": {
        "id": "stxEozV3Vnfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getRawData():\n",
        "\n",
        "  # Read data\n",
        "  drug = pd.read_csv('/content/drive/Shareddrives/CMPE255/data/DAWN-2011-DS0001-data-excel.tsv', sep = '\\t', header = 0)\n",
        "\n",
        "  # Split dataset into demographic information, case related information and drug related information for easy processing\n",
        "  demographic_info_df = drug[['CASEID', 'METRO', 'AGECAT', 'SEX', 'RACE']]\n",
        "\n",
        "  case_info_df = drug[['CASEID', 'YEAR', 'QUARTER', 'DAYPART', 'NUMSUBS', 'CASETYPE', 'DISPOSITION']]\n",
        "\n",
        "  # Create drug related information df\n",
        "  drug_info_cols = ['CASEID'] \n",
        "  for i in range(1, 23):\n",
        "    drug_info_cols.append('DRUGID_' + str(i))\n",
        "    drug_info_cols.append('CATID_1_' + str(i))\n",
        "    drug_info_cols.append('CATID_2_' + str(i))\n",
        "    drug_info_cols.append('CATID_3_' + str(i))\n",
        "    drug_info_cols.append('ROUTE_' + str(i))\n",
        "    drug_info_cols.append('TOXTEST_' + str(i))\n",
        "    drug_info_cols.append('sdled_1_' + str(i))\n",
        "    drug_info_cols.append('sdled_2_' + str(i))\n",
        "    drug_info_cols.append('sdled_3_' + str(i))\n",
        "    drug_info_cols.append('sdled_4_' + str(i))\n",
        "    drug_info_cols.append('sdled_5_' + str(i))\n",
        "    drug_info_cols.append('sdled_6_' + str(i))\n",
        "\n",
        "  # Append remaining columns\n",
        "  drug_info_cols.append('ALCOHOL')\n",
        "  drug_info_cols.append('NONALCILL')\n",
        "  drug_info_cols.append('PHARMA')\n",
        "  drug_info_cols.append('NONMEDPHARMA')\n",
        "  drug_info_cols.append('ALLABUSE')\n",
        "\n",
        "  drug_info_df = drug[drug_info_cols]\n",
        "\n",
        "  return demographic_info_df, case_info_df, drug_info_df"
      ],
      "metadata": {
        "id": "HesHYxqcVtlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was found in the EDA phase that NUMSUBS has outliers, i.e. the number of drugs reported by the patients. NUMSUBS > 3 lie outside (Q3+1.5*IQR) and therefore those records are removed from the dataset. Along with that sdled_5 and sdled_6 columns for each drug are also removed since it is not applicable for more than 95% of the records."
      ],
      "metadata": {
        "id": "15smrDzcVlgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def removeOutliers(case_info_df, drug_info_df):\n",
        "\n",
        "  # Remove outliers from numsubs column using IQR method\n",
        "  q25, q75 = np.percentile(case_info_df['NUMSUBS'], 25), np.percentile(case_info_df['NUMSUBS'], 75)\n",
        "  iqr = q75 - q25\n",
        "  cut_off = iqr * 1.5\n",
        "  lower, upper = q25 - cut_off, q75 + cut_off\n",
        "  case_info_df = case_info_df[(case_info_df.NUMSUBS >= lower) & (case_info_df.NUMSUBS <= upper)]\n",
        "\n",
        "  # Filter rows from drug_info_df as per the CASEID in case_info_df after outliers are removed\n",
        "  drug_info_df_temp = drug_info_df[drug_info_df.CASEID.isin(list(case_info_df.CASEID))]\n",
        "  cols = ['CASEID', 'DRUGID_1', 'CATID_1_1', 'CATID_2_1', 'CATID_3_1', 'ROUTE_1', 'TOXTEST_1', 'sdled_1_1', 'sdled_2_1', 'sdled_3_1', 'sdled_4_1', \\\n",
        "          'DRUGID_2', 'CATID_1_2', 'CATID_2_2', 'CATID_3_2', 'ROUTE_2', 'TOXTEST_2', 'sdled_1_2', 'sdled_2_2', 'sdled_3_2', 'sdled_4_2', \\\n",
        "          'DRUGID_3', 'CATID_1_3', 'CATID_2_3', 'CATID_3_3', 'ROUTE_3', 'TOXTEST_3', 'sdled_1_3', 'sdled_2_3', 'sdled_3_3', 'sdled_4_3', \\\n",
        "          'ALCOHOL', 'NONALCILL', 'PHARMA', 'NONMEDPHARMA', 'ALLABUSE']\n",
        "\n",
        "  # After removing numsubs outliers, the max number of drugs reported in any case is 3. therefore, elimiate all others and fit the other in the available 3 columns\n",
        "  reduced_drug_info_df = pd.DataFrame(columns = cols)\n",
        "  for idx, r in drug_info_df_temp.iterrows():\n",
        "    row = [r.CASEID]\n",
        "    num = 1\n",
        "    for i in range(1, 23):\n",
        "        if r['DRUGID_' + str(i)] != -7:\n",
        "          row.append(r['DRUGID_' + str(num)])\n",
        "          row.append(r['CATID_1_' + str(num)])\n",
        "          row.append(r['CATID_2_' + str(num)])\n",
        "          row.append(r['CATID_3_' + str(num)])\n",
        "          row.append(r['ROUTE_' + str(num)])\n",
        "          row.append(r['TOXTEST_' + str(num)])\n",
        "          row.append(r['sdled_1_' + str(num)])\n",
        "          row.append(r['sdled_2_' + str(num)])\n",
        "          row.append(r['sdled_3_' + str(num)])\n",
        "          row.append(r['sdled_4_' + str(num)])\n",
        "        \n",
        "          num += 1\n",
        "\n",
        "    # For records containing less than 3 numsubs, append -7 for the remaining ones\n",
        "    if num < 4:\n",
        "        missing = [-7] * ((4-num)*10)\n",
        "        row.extend(missing)\n",
        "\n",
        "    row.append(r['ALCOHOL'])\n",
        "    row.append(r['NONALCILL'])\n",
        "    row.append(r['PHARMA'])\n",
        "    row.append(r['NONMEDPHARMA'])\n",
        "    row.append(r['ALLABUSE'])\n",
        "\n",
        "    # Append to landing df\n",
        "    reduced_drug_info_df = reduced_drug_info_df.append(pd.DataFrame([row], columns = cols))\n",
        "\n",
        "  return case_info_df, reduced_drug_info_df"
      ],
      "metadata": {
        "id": "UVabLtgkW9PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains negative values that represent the following:\n",
        "*   -7 : Not applicable\n",
        "*   -8 : Not documented \n",
        "*   -9 : Missing\n",
        "\n",
        "All these are replaced by 0 as these values cannot be imputed or estimated by interpolation. By substituting them as 0 tells the classification model to treat them as one category of variables.\n"
      ],
      "metadata": {
        "id": "W5Iw9DxK5tpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanData(reduced_drug_info_df, case_info_df, demographic_info_df):\n",
        "  \n",
        "  # Merge all 3 dataframes together to generate final dataframe\n",
        "  final_df = (reduced_drug_info_df.merge(case_info_df, on = ['CASEID'], how = 'left')).merge(demographic_info_df, on = ['CASEID'], how = 'left')\n",
        "\n",
        "  # Replace -7, -8 and -9 with 0\n",
        "  final_df = final_df.replace({-7:0, -8:0, -9:0})\n",
        "\n",
        "  # Drop columns that are not required\n",
        "  final_df = final_df.drop(['YEAR', 'QUARTER', 'DAYPART', 'NUMSUBS', 'DISPOSITION'], axis = 1)\n",
        "\n",
        "  # Save new dataframe which will be used for further processing and model training\n",
        "  final_df.to_csv('/content/drive/Shareddrives/CMPE255/data/final_drug_data.csv')\n",
        "\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "mj4XAfAQZ_4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createMappingsDf(final_df, cols):\n",
        "\n",
        "  # Mappings dataframe for drug input from user on dashboard to get catids and sdleds for inputted drug\n",
        "  columns = ['DRUGID', 'CATID_1', 'CATID_2', 'CATID_3', 'sdled_1', 'sdled_2', 'sdled_3', 'sdled_4']\n",
        "  df_ = final_df[['DRUGID_1', 'CATID_1_1', 'CATID_2_1', 'CATID_3_1', 'sdled_1_1', 'sdled_2_1', 'sdled_3_1', 'sdled_4_1']]\n",
        "\n",
        "  prominent_drugs = [1255, 1254, 1253, 865, 2420, 21, 2427, 2343, 1016, 505, 85, 152]\n",
        "\n",
        "  cat_sdled_df = df_[df_.DRUGID_1.isin(prominent_drugs)].drop_duplicates().reset_index(drop = True)\n",
        "  cat_sdled_df.columns = columns\n",
        "\n",
        "  # Save dataframe\n",
        "  cat_sdled_df.to_csv('/content/drive/Shareddrives/CMPE255/data/cat_sdled_mapping.csv')"
      ],
      "metadata": {
        "id": "fDTIaL9xf0mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The datasets for predicting the casetype and abuse type contain all categorical features. Since the cardinality is very high, we perform hash encoding on the high cardinality columns and one hot encoding on the low cardinality columns. This is done so that categorical columns are treated as categories by the classificaiton models.\n",
        "\n",
        "*   Hashencoding -  Hashencoding is a process of converting categorical features with very high cardinality into numerical features. Hash encoders hash every value in the feature column and the hash value determines the bucket that the value falls into. By taking 7 buckets, we encode all drug related columns such that every feature is expanded into 7 columns with binary data. \n",
        "*   For demographic and some drug related columns, onehot encoding is followed as the number of categories is small. One hot encoding ensures no information is lost, unlike hash encoding where information loss occurs due to hashing collisions. However, hash encoding offers a compressed encoding that is computationally efficient.\n"
      ],
      "metadata": {
        "id": "Is0xAMkVWdnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encodingCasetype(df):\n",
        "\n",
        "  # Getting casetypes that are not others(8) in train and those that are 8 in test\n",
        "  case_train = df[df['CASETYPE'] != 8]\n",
        "  case_test = df[df['CASETYPE'] == 8]\n",
        "\n",
        "  # Getting data for one hot encoding\n",
        "  demo_cols = ['METRO', 'AGECAT', 'SEX', 'RACE', 'CASETYPE', 'ROUTE_1', 'TOXTEST_1', 'ROUTE_2', 'TOXTEST_2', 'ROUTE_3', 'TOXTEST_3', 'ALLABUSE','NONALCILL','ALCOHOL','NONMEDPHARMA', 'PHARMA']\n",
        "  case_train_one_hot = case_train[demo_cols]\n",
        "  case_test_one_hot = case_test[demo_cols]\n",
        "\n",
        "  # Getting drug data for category hash encoding\n",
        "  drug_cols = ['DRUGID_1', 'CATID_1_1', 'CATID_2_1', 'CATID_3_1', 'sdled_1_1', 'sdled_2_1', 'sdled_3_1', 'sdled_4_1', \n",
        "        'DRUGID_2', 'CATID_1_2', 'CATID_2_2', 'CATID_3_2', 'sdled_1_2', 'sdled_2_2', 'sdled_3_2', 'sdled_4_2',\n",
        "        'DRUGID_3', 'CATID_1_3', 'CATID_2_3', 'CATID_3_3', 'sdled_1_3', 'sdled_2_3', 'sdled_3_3', 'sdled_4_3']\n",
        "  case_train_drug = case_train[drug_cols]\n",
        "  case_test_drug = case_test[drug_cols]\n",
        "  \n",
        "\n",
        "  def oneHotEncode(df):\n",
        "    onehotencode = pd.DataFrame()\n",
        "      \n",
        "    # Onehot code\n",
        "    for feature in df.columns:  \n",
        "      demo_encoded = pd.get_dummies(df[feature], prefix = feature)\n",
        "      onehotencode = pd.concat([onehotencode, demo_encoded], axis = 1)\n",
        "\n",
        "    return onehotencode\n",
        "\n",
        "\n",
        "  def convertToCategory(drug_df):\n",
        "    for c in drug_df.columns:\n",
        "      drug_df[c] = drug_df[c].astype('category')\n",
        "\n",
        "    return drug_df\n",
        "\n",
        "\n",
        "  def hashEncode(category_case_train_drug,c ategory_case_test_drug):\n",
        "    drug_trainhashencoding = pd.DataFrame()\n",
        "    drug_testhashencoding = pd.DataFrame()\n",
        "\n",
        "    # hash encode feature wise\n",
        "    for feature in category_case_train_drug.columns:\n",
        "      # Hashing encoder\n",
        "      encoder = ce.HashingEncoder(cols=feature)\n",
        "      encoder.fit(category_case_train_drug[feature])\n",
        "\n",
        "      # Save encoding object\n",
        "      pickle.dump(encoder,open('/content/drive/Shareddrives/CMPE255/pickles'+feature+'.pkl', 'wb'))\n",
        "\n",
        "      # Transform train and test set with hashing encoder object\n",
        "      case_train_drug_hash = encoder.transform(category_case_train_drug)\n",
        "      case_test_drug_hash = encoder.transform(category_case_test_drug)\n",
        "\n",
        "      # Combine\n",
        "      drug_trainhashencoding = pd.concat([case_train_drug_hash, drug_trainhashencoding], axis = 1)\n",
        "      drug_testhashencoding = pd.concat([case_test_drug_hash, drug_testhashencoding], axis = 1)\n",
        "\n",
        "    return drug_trainhashencoding,drug_testhashencoding\n",
        "\n",
        "  # Onehot encode demographic information\n",
        "  case_train_onehot = oneHotEncode(case_train_one_hot).reset_index(drop = True)\n",
        "  case_test_onehot = oneHotEncode(case_test_one_hot).reset_index(drop = True)\n",
        "\n",
        "  # Convert datatype to category for hash encoding\n",
        "  category_case_train_drug = convertToCategory(case_train_drug)\n",
        "  category_case_test_drug = convertToCategory(case_test_drug)\n",
        "\n",
        "  # Hashencode drug information related columns\n",
        "  hashencode_case_train, hashencode_case_test = hashEncode(category_case_train_drug, category_case_test_drug)\n",
        "  hashencode_case_train.reset_index(drop = True, inplace = True)\n",
        "  hashencode_case_test.reset_index(drop = True, inplace = True)\n",
        "\n",
        "  # Concat all three dataframes to generate final df\n",
        "  final_case_train = pd.concat([case_train_onehot, hashencode_case_train], axis = 1)\n",
        "  final_case_test = pd.concat([case_test_onehot, hashencode_case_train], axis = 1)\n",
        "\n",
        "  # Save dataframe\n",
        "  final_case_train.to_csv('/content/drive/Shareddrives/CMPE255/data/encodings/encodedCasetype_train.csv')\n",
        "  final_case_test.to_csv('/content/drive/Shareddrives/CMPE255/data/encodings/encodedCasetype_test.csv')\n",
        "\n",
        "  return final_case_train, final_case_test"
      ],
      "metadata": {
        "id": "Luc85638KWNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encodingAbuse(df):\n",
        "\n",
        "  # Train test split on cleaned df\n",
        "  X = df.drop(['ALLABUSE','NONALCILL','ALCOHOL','NONMEDPHARMA', 'PHARMA'], axis = 1)\n",
        "  y = df['ALLABUSE','NONALCILL','ALCOHOL','NONMEDPHARMA', 'PHARMA']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 42)\n",
        "\n",
        "  # Getting data for one hot encoding\n",
        "  demo_cols = ['METRO', 'AGECAT', 'SEX', 'RACE', 'PHARMA', 'CASETYPE', 'ROUTE_1', 'TOXTEST_1', 'ROUTE_2', 'TOXTEST_2', 'ROUTE_3', 'TOXTEST_3']\n",
        "  case_train_one_hot = X_train[demo_cols]\n",
        "  case_test_one_hot = X_test[demo_cols]\n",
        "\n",
        "  # Getting drug data for category hash encoding\n",
        "  drug_cols = ['DRUGID_1', 'CATID_1_1', 'CATID_2_1', 'CATID_3_1', 'sdled_1_1', 'sdled_2_1', 'sdled_3_1', 'sdled_4_1',\n",
        "        'DRUGID_2', 'CATID_1_2', 'CATID_2_2', 'CATID_3_2', 'sdled_1_2', 'sdled_2_2', 'sdled_3_2', 'sdled_4_2',\n",
        "        'DRUGID_3', 'CATID_1_3', 'CATID_2_3', 'CATID_3_3', 'sdled_1_3', 'sdled_2_3', 'sdled_3_3', 'sdled_4_3']\n",
        "  case_train_drug = X_train[drug_cols]\n",
        "  case_test_drug = X_test[drug_cols]\n",
        "\n",
        "\n",
        "  def oneHotEncode(df):\n",
        "    onehotencode = pd.DataFrame()\n",
        "      \n",
        "    # Onehot code\n",
        "    for feature in df.columns:  \n",
        "      demo_encoded = pd.get_dummies(df[feature], prefix = feature)\n",
        "      onehotencode = pd.concat([onehotencode, demo_encoded], axis = 1)\n",
        "\n",
        "    return onehotencode\n",
        "\n",
        "\n",
        "  def convertToCategory(drug_df):\n",
        "    for c in drug_df.columns:\n",
        "      drug_df[c] = drug_df[c].astype('category')\n",
        "\n",
        "    return drug_df\n",
        "\n",
        "\n",
        "  def hashEncode(category_case_train_drug, c ategory_case_test_drug):\n",
        "    drug_trainhashencoding = pd.DataFrame()\n",
        "    drug_testhashencoding = pd.DataFrame()\n",
        "\n",
        "    # hash encode feature wise\n",
        "    for feature in category_case_train_drug.columns:\n",
        "      # Hashing encoder\n",
        "      encoder = ce.HashingEncoder(cols = feature)\n",
        "      encoder.fit(category_case_train_drug[feature])\n",
        "\n",
        "      # Transform train and test set with hashing encoder object\n",
        "      case_train_drug_hash = encoder.transform(category_case_train_drug)\n",
        "      case_test_drug_hash = encoder.transform(category_case_test_drug)\n",
        "\n",
        "      # Combine\n",
        "      drug_trainhashencoding = pd.concat([case_train_drug_hash, drug_trainhashencoding], axis = 1)\n",
        "      drug_testhashencoding = pd.concat([case_test_drug_hash, drug_testhashencoding], axis = 1)\n",
        "\n",
        "    return drug_trainhashencoding,drug_testhashencoding\n",
        "\n",
        "\n",
        "  # Columns that won't be encoded\n",
        "  data_to_concatenate_last_train = y_train[['ALLABUSE','NONALCILL','ALCOHOL','NONMEDPHARMA','PHARMA']].reset_index(drop = True, inplace = True)\n",
        "  data_to_concatenate_last_test = y_test[['ALLABUSE','NONALCILL','ALCOHOL','NONMEDPHARMA', 'PHARMA']].reset_index(drop = True, inplace = True)\n",
        "\n",
        "  # One hot encode demographic columns\n",
        "  case_train_onehot = oneHotEncode(case_train_one_hot).reset_index(drop = True)\n",
        "  case_test_onehot = oneHotEncode(case_test_one_hot).reset_index(drop = True)\n",
        "\n",
        "  # Convert datatype to category\n",
        "  category_case_train_drug = convertToCategory(case_train_drug)\n",
        "  category_case_test_drug = convertToCategory(case_test_drug)\n",
        "\n",
        "  # Hashencode drug related columns\n",
        "  hashencode_case_train, hashencode_case_test = hashEncode(category_case_train_drug, category_case_test_drug)\n",
        "  hashencode_case_train.reset_index(drop = True, inplace = True)\n",
        "  hashencode_case_test.reset_index(drop = True, inplace = True)\n",
        "\n",
        "  # Merge all encodings\n",
        "  final_abuse_train = pd.concat([case_train_onehot, hashencode_case_train, data_to_concatenate_last_train], axis = 1)\n",
        "  final_abuse_test = pd.concat([case_test_onehot, hashencode_case_train, data_to_concatenate_last_test], axis = 1)\n",
        "\n",
        "  # Save dataframe\n",
        "  final_abuse_train.to_csv('/content/drive/Shareddrives/CMPE255/data/encodings/encodedAbuse_train.csv')\n",
        "  final_abuse_test.to_csv('/content/drive/Shareddrives/CMPE255/data/encodings/encodedAbuse_test.csv')\n",
        "\n",
        "  return final_abuse_train, final_abuse_test"
      ],
      "metadata": {
        "id": "ubKF73wHgHTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the encoding, the number of feature columns increases drastically. Thus, reducing the dimensionality becomes an inevident step in the processing pipeline. The dimensionality of the datasets is reducing using PCA and MCA.\n",
        "*   For dataset A, the encodings are then combined and provided to Principal Component Analysis. PCA finds the principal components that explain 80% variance in the data. The resultant dataset has reduced dimensions with minimal information loss which is then input to the model for multilabel classification of the type of abuse. \n",
        "*   For dataset C, the encoded data is passed through MCA (Multiple Correpsondence Analysis) which is similar to PCA but is specific to categorical data. This too reduces the dimensionality of the dataset.\n"
      ],
      "metadata": {
        "id": "o5HrOp9uXIr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pcaAbuse(train, test):\n",
        "\n",
        "  cols = ['ALLABUSE', 'NONALCILL', 'ALCOHOL', 'NONMEDPHARMA', 'PHARMA']\n",
        "\n",
        "  # Separate X and y\n",
        "  X_train = train.drop(cols, axis = 1)\n",
        "  y_train = train[cols]\n",
        "  X_test = test.drop(cols, axis = 1)\n",
        "  y_test = test[cols]\n",
        "\n",
        "  # PCA\n",
        "  pca = PCA(n_components = 0.8).fit(X_train)\n",
        "  X_train_pca = pca.transform(X_train)\n",
        "  X_test_pca = pca.transform(X_test)\n",
        "\n",
        "  # Concatenate X and y columns for both train and test set\n",
        "  X_train_pca_df = pd.DataFrame(data = X_train_pca)\n",
        "  X_train_pca_df.reset_index(drop = True, inplace = True)\n",
        "  y_train.reset_index(drop = True, inplace = True)\n",
        "  X_train_pca = pd.concat([X_train_pca_df, y_train], axis = 1)\n",
        "\n",
        "  X_test_pca_df = pd.DataFrame(data = X_test_pca)\n",
        "  X_test_pca_df.reset_index(drop = True, inplace = True)\n",
        "  y_test.reset_index(drop = True, inplace = True)\n",
        "  X_test_pca = pd.concat([X_test_pca_df, y_test], axis = 1)\n",
        "\n",
        "  # Dump pca object\n",
        "  pickle.dump(pca, open('/content/drive/Shareddrives/CMPE255/pickles/pca_abuse_obj.pkl', 'wb'))\n",
        "\n",
        "  # Save df\n",
        "  X_train_pca.to_csv('/content/drive/Shareddrives/CMPE255/data/dimensionality_reduction/X_train_abuse.csv')\n",
        "  X_test_pca.to_csv('/content/drive/Shareddrives/CMPE255/data/dimensionality_reduction/X_test_abuse.csv')   "
      ],
      "metadata": {
        "id": "bPhVhINbJgWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pcaCasetype(train, test):\n",
        "\n",
        "  train_cols = ['CASETYPE_1', 'CASETYPE_2', 'CASETYPE_3', 'CASETYPE_4', 'CASETYPE_5', 'CASETYPE_6', 'CASETYPE_7']\n",
        "  test_cols = ['CASETYPE_8']\n",
        "\n",
        "  # Separate X and y\n",
        "  X_train = train.drop(train_cols, axis = 1)\n",
        "  y_train = traintrain_cols]\n",
        "  X_test = test.drop(test_cols, axis = 1)\n",
        "  y_test = test[test_cols]\n",
        "\n",
        "  # Map value for MCA\n",
        "  X_train.replace({0: 'False', 1: 'True'}, inplace = True)\n",
        "  X_test.replace({0: 'False', 1: 'True'}, inplace = True)\n",
        "\n",
        "  # Add AALLABUSE_0 column since ALLABUSE does not take value 0 for CASETYPE == 8\n",
        "  X_test.insert(38, 'ALLABUSE_0', 0)\n",
        "\n",
        "  # PCA\n",
        "  pca = PCA(n_components = 0.8).fit(X_train)\n",
        "  X_train_pca = pca.transform(X_train)\n",
        "  X_test_pca = pca.transform(X_test)\n",
        "\n",
        "  # Concatenate X and y columns for both train and test set\n",
        "  X_train_pca_df = pd.DataFrame(data = X_train_pca)\n",
        "  X_train_pca_df.reset_index(drop = True, inplace = True)\n",
        "  y_train.reset_index(drop = True, inplace = True)\n",
        "  X_train_pca = pd.concat([X_train_pca_df, y_train], axis = 1)\n",
        "\n",
        "  X_test_pca_df = pd.DataFrame(data = X_test_pca)\n",
        "  X_test_pca_df.reset_index(drop = True, inplace = True)\n",
        "  y_test.reset_index(drop = True, inplace = True)\n",
        "  X_test_pca = pd.concat([X_test_pca_df, y_test], axis = 1)\n",
        "\n",
        "  # Save dataframes\n",
        "  X_train_pca.to_csv('/content/drive/Shareddrives/CMPE255/data/dimensionality_reduction/X_train_casetype.csv')\n",
        "  X_test_pca.to_csv('/content/drive/Shareddrives/CMPE255/data/dimensionality_reduction/X_test_casetype.csv')"
      ],
      "metadata": {
        "id": "bHzizaXQ5BoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function calls for data preparation\n",
        "\n",
        "demographic_info_df, case_info_df, drug_info_df = getRawData()\n",
        "case_info_df, reduced_drug_info_df = removeOutliers(case_info_df, drug_info_df)\n",
        "final_df = cleanData(reduced_drug_info_df, case_info_df, demographic_info_df)\n",
        "createMappingsDf(final_df)\n",
        "\n",
        "encoded_abuse_train_df, encoded_abuse_test_df = encodingCasetype(final_df)\n",
        "encoded_casetype_train_df, encoded_casetype_test_df = encodingAbuse(final_df)\n",
        "\n",
        "pcaAbuse(encoded_abuse_train_df, encoded_abuse_test_df)\n",
        "pcaCasetype(encoded_casetype_train_df, encoded_casetype_test_df)"
      ],
      "metadata": {
        "id": "hT_pgIjzJg55"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}