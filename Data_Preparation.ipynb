{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananya-AJ/CMPE255-SafeDose/blob/main/Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**This colab provides a step-by-step data preparation pipeline for the project \n",
        "'Safe Dose'. The data preparation steps are functionalized and can be called sequentially to obtain the final processed dataset.**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KuFKC91CUBo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "!pip install category_encoders\n",
        "!pip install --user prince"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WqBEN8vHiI5D",
        "outputId": "bebeab0b-f80e-4b28-d88f-4f664a2ec774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting featurewiz\n",
            "  Using cached featurewiz-0.2.3-py3-none-any.whl (111 kB)\n",
            "Requirement already satisfied: distributed>=2021.11.0 in /usr/local/lib/python3.7/dist-packages (from featurewiz) (2022.2.0)\n",
            "Requirement already satisfied: dask>=2021.11.0 in /usr/local/lib/python3.7/dist-packages (from featurewiz) (2022.2.0)\n",
            "Collecting lightgbm>=3.2.1\n",
            "  Using cached lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "Requirement already satisfied: imbalanced-learn>=0.7 in /usr/local/lib/python3.7/dist-packages (from featurewiz) (0.8.1)\n",
            "Requirement already satisfied: category-encoders>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from featurewiz) (2.5.1.post0)\n",
            "Requirement already satisfied: feather-format>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from featurewiz) (0.4.1)\n",
            "Collecting Pillow>=9.0.0\n",
            "  Downloading Pillow-9.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from featurewiz) (1.0.2)\n",
            "Requirement already satisfied: fsspec>=0.3.3 in /usr/local/lib/python3.7/dist-packages (from featurewiz) (2022.11.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from featurewiz) (7.9.0)\n",
            "Requirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.7/dist-packages (from featurewiz) (1.3.5)\n",
            "Collecting xgboost>=1.5.1\n",
            "  Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl (255.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 255.9 MB 58 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.0 in /usr/local/lib/python3.7/dist-packages (from featurewiz) (4.64.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from featurewiz) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from featurewiz) (0.11.2)\n",
            "Collecting xlrd>=2.0.0\n",
            "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from featurewiz) (2.6.3)\n",
            "Collecting pyarrow~=7.0.0\n",
            "  Downloading pyarrow-7.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.7 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting jupyter\n",
            "  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category-encoders>=2.4.0->featurewiz) (1.21.6)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders>=2.4.0->featurewiz) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category-encoders>=2.4.0->featurewiz) (1.7.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category-encoders>=2.4.0->featurewiz) (0.12.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from dask>=2021.11.0->featurewiz) (6.0)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask>=2021.11.0->featurewiz) (1.5.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask>=2021.11.0->featurewiz) (0.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask>=2021.11.0->featurewiz) (21.3)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask>=2021.11.0->featurewiz) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.11.0->featurewiz) (2.4.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.11.0->featurewiz) (2.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.11.0->featurewiz) (2.11.3)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.11.0->featurewiz) (7.1.2)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.11.0->featurewiz) (1.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.11.0->featurewiz) (57.4.0)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.11.0->featurewiz) (6.0.4)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.11.0->featurewiz) (5.4.8)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.11.0->featurewiz) (1.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn>=0.7->featurewiz) (1.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.2.1->featurewiz) (0.38.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->dask>=2021.11.0->featurewiz) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.4->featurewiz) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.4->featurewiz) (2.8.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask>=2021.11.0->featurewiz) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category-encoders>=2.4.0->featurewiz) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->featurewiz) (3.1.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2021.11.0->featurewiz) (1.0.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->featurewiz) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->featurewiz) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->featurewiz) (2.0.10)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->featurewiz) (5.1.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->featurewiz) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->featurewiz) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->featurewiz) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->featurewiz) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->featurewiz) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2021.11.0->featurewiz) (2.0.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->featurewiz) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->featurewiz) (7.7.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->featurewiz) (6.1.0)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 61.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->featurewiz) (5.7.16)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->featurewiz) (5.3.4)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->featurewiz) (6.1.12)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->featurewiz) (3.0.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->featurewiz) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->featurewiz) (3.6.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->featurewiz) (23.2.1)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->featurewiz) (4.11.2)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->featurewiz) (0.13.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->featurewiz) (0.15.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->featurewiz) (5.7.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->featurewiz) (1.8.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->featurewiz) (0.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->featurewiz) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->featurewiz) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->featurewiz) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->featurewiz) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->featurewiz) (5.0.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->featurewiz) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->featurewiz) (4.13.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->featurewiz) (2.16.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook->jupyter->featurewiz) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook->jupyter->featurewiz) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->featurewiz) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->featurewiz) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->featurewiz) (0.19.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->featurewiz) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->featurewiz) (0.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->featurewiz) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->featurewiz) (0.11.0)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: jedi, qtpy, qtconsole, pyarrow, xlrd, xgboost, Pillow, lightgbm, jupyter, featurewiz\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 9.0.0\n",
            "    Uninstalling pyarrow-9.0.0:\n",
            "      Successfully uninstalled pyarrow-9.0.0\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed Pillow-9.3.0 featurewiz-0.2.3 jedi-0.18.2 jupyter-1.0.0 lightgbm-3.3.3 pyarrow-7.0.0 qtconsole-5.4.0 qtpy-2.3.0 xgboost-1.6.2 xlrd-2.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.7/dist-packages (2.5.1.post0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.3.5)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.7.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.12.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.5->category_encoders) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oYNesbFVeop"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import category_encoders as ce\n",
        "from sklearn.decomposition import PCA\n",
        "import prince\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFDeSOGuVlqd",
        "outputId": "374748b0-56de-43f3-ba01-414706635786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import raw data and segregate data into demographic information, case related information and drug related information for easier processing."
      ],
      "metadata": {
        "id": "stxEozV3Vnfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getRawData():\n",
        "\n",
        "  # Read data\n",
        "  drug = pd.read_csv('/content/drive/Shareddrives/CMPE255/data/DAWN-2011-DS0001-data-excel.tsv', sep = '\\t', header = 0)\n",
        "\n",
        "  # Split dataset into demographic information, case related information and drug related information for easy processing\n",
        "  demographic_info_df = drug[['CASEID', 'METRO', 'AGECAT', 'SEX', 'RACE']]\n",
        "\n",
        "  case_info_df = drug[['CASEID', 'CASEWGT', 'YEAR', 'QUARTER', 'DAYPART', 'NUMSUBS', 'CASETYPE', 'DISPOSITION', 'ALLABUSE']]\n",
        "\n",
        "  # Create drug related information df\n",
        "  drug_info_cols = ['CASEID'] \n",
        "  for i in range(1, 23):\n",
        "    drug_info_cols.append('DRUGID_' + str(i))\n",
        "    drug_info_cols.append('CATID_1_' + str(i))\n",
        "    drug_info_cols.append('CATID_2_' + str(i))\n",
        "    drug_info_cols.append('CATID_3_' + str(i))\n",
        "    drug_info_cols.append('ROUTE_' + str(i))\n",
        "    drug_info_cols.append('TOXTEST_' + str(i))\n",
        "    drug_info_cols.append('sdled_1_' + str(i))\n",
        "    drug_info_cols.append('sdled_2_' + str(i))\n",
        "    drug_info_cols.append('sdled_3_' + str(i))\n",
        "    drug_info_cols.append('sdled_4_' + str(i))\n",
        "    drug_info_cols.append('sdled_5_' + str(i))\n",
        "    drug_info_cols.append('sdled_6_' + str(i))\n",
        "\n",
        "  # Append remaining columns\n",
        "  drug_info_cols.append('ALCOHOL')\n",
        "  drug_info_cols.append('NONALCILL')\n",
        "  drug_info_cols.append('PHARMA')\n",
        "  drug_info_cols.append('NONMEDPHARMA')\n",
        "\n",
        "  drug_info_df = drug[drug_info_cols]\n",
        "\n",
        "  return demographic_info_df, case_info_df, drug_info_df"
      ],
      "metadata": {
        "id": "HesHYxqcVtlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was found in the EDA phase that NUMSUBS has outliers, i.e. the number of drugs reported by the patients. NUMSUBS > 3 lie outside (Q3+1.5*IQR) and therefore those records are removed from the dataset. Along with that sdled_5 and sdled_6 columns for each drug are also removed since it is not applicable for more than 95% of the records."
      ],
      "metadata": {
        "id": "15smrDzcVlgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def removeOutliers(case_info_df, drug_info_df):\n",
        "\n",
        "  # Remove outliers from numsubs column using IQR method\n",
        "  q25, q75 = np.percentile(case_info_df['NUMSUBS'], 25), np.percentile(case_info_df['NUMSUBS'], 75)\n",
        "  iqr = q75 - q25\n",
        "  cut_off = iqr * 1.5\n",
        "  lower, upper = q25 - cut_off, q75 + cut_off\n",
        "  case_info_df = case_info_df[(case_info_df.NUMSUBS >= lower) & (case_info_df.NUMSUBS <= upper)]\n",
        "\n",
        "  # Filter rows from drug_info_df as per the CASEID in case_info_df after outliers are removed\n",
        "  drug_info_df_temp = drug_info_df[drug_info_df.CASEID.isin(list(case_info_df.CASEID))]\n",
        "  cols = ['CASEID', 'DRUGID_1', 'CATID_1_1', 'CATID_2_1', 'CATID_3_1', 'ROUTE_1', 'TOXTEST_1', 'sdled_1_1', 'sdled_2_1', 'sdled_3_1', 'sdled_4_1', \\\n",
        "          'DRUGID_2', 'CATID_1_2', 'CATID_2_2', 'CATID_3_2', 'ROUTE_2', 'TOXTEST_2', 'sdled_1_2', 'sdled_2_2', 'sdled_3_2', 'sdled_4_2', \\\n",
        "          'DRUGID_3', 'CATID_1_3', 'CATID_2_3', 'CATID_3_3', 'ROUTE_3', 'TOXTEST_3', 'sdled_1_3', 'sdled_2_3', 'sdled_3_3', 'sdled_4_3', \\\n",
        "          'ALCOHOL', 'NONALCILL', 'PHARMA', 'NONMEDPHARMA']\n",
        "\n",
        "  # After removing numsubs outliers, the max number of drugs reported in any case is 3. therefore, elimiate all others and fit the other in the available 3 columns\n",
        "  reduced_drug_info_df = pd.DataFrame(columns = cols)\n",
        "  for idx, r in drug_info_df_temp.iterrows():\n",
        "    row = [r.CASEID]\n",
        "    num = 1\n",
        "    for i in range(1, 23):\n",
        "        if r['DRUGID_' + str(i)] != -7:\n",
        "          row.append(r['DRUGID_' + str(num)])\n",
        "          row.append(r['CATID_1_' + str(num)])\n",
        "          row.append(r['CATID_2_' + str(num)])\n",
        "          row.append(r['CATID_3_' + str(num)])\n",
        "          row.append(r['ROUTE_' + str(num)])\n",
        "          row.append(r['TOXTEST_' + str(num)])\n",
        "          row.append(r['sdled_1_' + str(num)])\n",
        "          row.append(r['sdled_2_' + str(num)])\n",
        "          row.append(r['sdled_3_' + str(num)])\n",
        "          row.append(r['sdled_4_' + str(num)])\n",
        "        \n",
        "          num += 1\n",
        "\n",
        "    # For records containing less than 3 numsubs, append -7 for the remaining ones\n",
        "    if num < 4:\n",
        "        missing = [-7] * ((4-num)*10)\n",
        "        row.extend(missing)\n",
        "\n",
        "    row.append(r['ALCOHOL'])\n",
        "    row.append(r['NONALCILL'])\n",
        "    row.append(r['PHARMA'])\n",
        "    row.append(r['NONMEDPHARMA'])\n",
        "\n",
        "    # Append to landing df\n",
        "    reduced_drug_info_df = reduced_drug_info_df.append(pd.DataFrame([row], columns = cols))\n",
        "\n",
        "  return case_info_df, reduced_drug_info_df"
      ],
      "metadata": {
        "id": "UVabLtgkW9PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains negative values that represent the following:\n",
        "*   -7 : Not applicable\n",
        "*   -8 : Not documented \n",
        "*   -9 : Missing\n",
        "\n",
        "All these are replaced by 0 as these values cannot be imputed or estimated by interpolation. By substituting them as 0 tells the classification model to treat them as one category of variables.\n"
      ],
      "metadata": {
        "id": "W5Iw9DxK5tpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanData(reduced_drug_info_df, case_info_df, demographic_info_df):\n",
        "  \n",
        "  # Merge all 3 dataframes together to generate final dataframe\n",
        "  final_df = (reduced_drug_info_df.merge(case_info_df, on = ['CASEID'], how = 'left')).merge(demographic_info_df, on = ['CASEID'], how = 'left')\n",
        "\n",
        "  # Replace -7, -8 and -9 with 0\n",
        "  final_df = final_df.replace({-7:0, -8:0, -9:0})\n",
        "\n",
        "  # Drop columns that are not required\n",
        "  final_df = final_df.drop(['YEAR', 'QUARTER', 'DAYPART', 'NUMSUBS', 'DISPOSITION'], axis = 1)\n",
        "\n",
        "  # Save new dataframe which will be used for further processing and model training\n",
        "  final_df.to_csv('/content/drive/Shareddrives/CMPE255/data/final_drug_data.csv')\n",
        "\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "mj4XAfAQZ_4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createMappingsDf(final_df, cols):\n",
        "\n",
        "  # Mappings dataframe for drug input from user on dashboard to get catids and sdleds for inputted drug\n",
        "  columns = ['DRUGID', 'CATID_1', 'CATID_2', 'CATID_3', 'sdled_1', 'sdled_2', 'sdled_3', 'sdled_4']\n",
        "  df_ = final_df[['DRUGID_1', 'CATID_1_1', 'CATID_2_1', 'CATID_3_1', 'sdled_1_1', 'sdled_2_1', 'sdled_3_1', 'sdled_4_1']]\n",
        "\n",
        "  prominent_drugs = [1255, 1254, 1253, 865, 2420, 21, 2427, 2343, 1016, 505, 85, 152]\n",
        "\n",
        "  cat_sdled_df = df_[df_.DRUGID_1.isin(prominent_drugs)].drop_duplicates().reset_index(drop = True)\n",
        "  cat_sdled_df.columns = columns\n",
        "\n",
        "  # Save dataframe\n",
        "  cat_sdled_df.to_csv('/content/drive/Shareddrives/CMPE255/data/cat_sdled_mapping.csv')"
      ],
      "metadata": {
        "id": "fDTIaL9xf0mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The datasets for predicting the casetype and abuse type contain all categorical features. Since the cardinality is very high, we perform hash encoding on the high cardinality columns and one hot encoding on the low cardinality columns. This is done so that categorical columns are treated as categories by the classificaiton models.\n",
        "\n",
        "*   Hashencoding -  Hashencoding is a process of converting categorical features with very high cardinality into numerical features. Hash encoders hash every value in the feature column and the hash value determines the bucket that the value falls into. By taking 7 buckets, we encode all drug related columns such that every feature is expanded into 7 columns with binary data. \n",
        "*   For demographic and some drug related columns, onehot encoding is followed as the number of categories is small. One hot encoding ensures no information is lost, unlike hash encoding where information loss occurs due to hashing collisions. However, hash encoding offers a compressed encoding that is computationally efficient.\n"
      ],
      "metadata": {
        "id": "Is0xAMkVWdnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encodingCasetype(df):\n",
        "\n",
        "  # Getting casetypes that are not others(8) in train and those that are 8 in test\n",
        "  case_train = df[df['CASETYPE'] != 8]\n",
        "  case_test = df[df['CASETYPE'] == 8]\n",
        "\n",
        "  # Getting data for one hot encoding\n",
        "  demo_cols = ['METRO', 'AGECAT', 'SEX', 'RACE', 'PHARMA', 'CASETYPE', 'ROUTE_1', 'TOXTEST_1', 'ROUTE_2', 'TOXTEST_2', 'ROUTE_3', 'TOXTEST_3']\n",
        "  case_train_one_hot = case_train[demo_cols]\n",
        "  case_test_one_hot = case_test[demo_cols]\n",
        "\n",
        "  # Getting drug data for category hash encoding\n",
        "  drug_cols = ['DRUGID_1', 'CATID_1_1', 'CATID_2_1', 'CATID_3_1', 'sdled_1_1', 'sdled_2_1', 'sdled_3_1', 'sdled_4_1',\n",
        "        'DRUGID_2', 'CATID_1_2', 'CATID_2_2', 'CATID_3_2', 'sdled_1_2', 'sdled_2_2', 'sdled_3_2', 'sdled_4_2',\n",
        "        'DRUGID_3', 'CATID_1_3', 'CATID_2_3', 'CATID_3_3', 'sdled_1_3', 'sdled_2_3', 'sdled_3_3', 'sdled_4_3']\n",
        "  case_train_drug = case_train[drug_cols]\n",
        "  case_test_drug = case_test[drug_cols]\n",
        "  \n",
        "\n",
        "  def oneHotEncode(df):\n",
        "    onehotencode = pd.DataFrame()\n",
        "      \n",
        "    # Onehot code\n",
        "    for feature in df.columns:  \n",
        "      demo_encoded = pd.get_dummies(df[feature], prefix = feature)\n",
        "      onehotencode = pd.concat([onehotencode, demo_encoded], axis = 1)\n",
        "\n",
        "    return onehotencode\n",
        "\n",
        "\n",
        "  def convertToCategory(drug_df):\n",
        "    for c in drug_df.columns:\n",
        "      drug_df[c] = drug_df[c].astype('category')\n",
        "\n",
        "    return drug_df\n",
        "\n",
        "\n",
        "  def hashEncode(category_case_train_drug,c ategory_case_test_drug):\n",
        "    drug_trainhashencoding = pd.DataFrame()\n",
        "    drug_testhashencoding = pd.DataFrame()\n",
        "\n",
        "    # hash encode feature wise\n",
        "    for feature in category_case_train_drug.columns:\n",
        "      # Hashing encoder\n",
        "      encoder = ce.HashingEncoder(cols=feature)\n",
        "      encoder.fit(category_case_train_drug[feature])\n",
        "\n",
        "      # Save encoding object\n",
        "      pickle.dump(encoder,open('/content/drive/Shareddrives/CMPE255/pickles'+feature+'.pkl', 'wb'))\n",
        "\n",
        "      # Transform train and test set with hashing encoder object\n",
        "      case_train_drug_hash = encoder.transform(category_case_train_drug)\n",
        "      case_test_drug_hash = encoder.transform(category_case_test_drug)\n",
        "\n",
        "      # Combine\n",
        "      drug_trainhashencoding = pd.concat([case_train_drug_hash, drug_trainhashencoding], axis = 1)\n",
        "      drug_testhashencoding = pd.concat([case_test_drug_hash, drug_testhashencoding], axis = 1)\n",
        "\n",
        "    return drug_trainhashencoding,drug_testhashencoding\n",
        "\n",
        "  # Onehot encode demographic information\n",
        "  case_train_onehot = oneHotEncode(case_train_one_hot).reset_index(drop = True)\n",
        "  case_test_onehot = oneHotEncode(case_test_one_hot).reset_index(drop = True)\n",
        "\n",
        "  # Convert datatype to category for hash encoding\n",
        "  category_case_train_drug = convertToCategory(case_train_drug)\n",
        "  category_case_test_drug = convertToCategory(case_test_drug)\n",
        "\n",
        "  # Hashencode drug information related columns\n",
        "  hashencode_case_train, hashencode_case_test = hashEncode(category_case_train_drug, category_case_test_drug)\n",
        "  hashencode_case_train.reset_index(drop = True, inplace = True)\n",
        "  hashencode_case_test.reset_index(drop = True, inplace = True)\n",
        "\n",
        "  # Concat all three dataframes to generate final df\n",
        "  final_case_train = pd.concat([case_train_onehot, hashencode_case_train], axis = 1)\n",
        "  final_case_test = pd.concat([case_test_onehot, hashencode_case_train], axis = 1)\n",
        "\n",
        "  # Save dataframe\n",
        "  final_case_train.to_csv('/content/drive/Shareddrives/CMPE255/data/encoded data/encodedCasetypeData_train.csv')\n",
        "  final_case_test.to_csv('/content/drive/Shareddrives/CMPE255/data/encoded data/encodedCasetypeData_test.csv')\n",
        "\n",
        "  return final_case_train, final_case_test"
      ],
      "metadata": {
        "id": "Luc85638KWNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encodingAbuse(df):\n",
        "\n",
        "  # Train test split on cleaned df\n",
        "  X = df.drop(['ALLABUSE','NONALCILL','ALCOHOL','NONMEDPHARMA', 'PHARMA'], axis = 1)\n",
        "  y = df['ALLABUSE','NONALCILL','ALCOHOL','NONMEDPHARMA', 'PHARMA']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 42)\n",
        "\n",
        "  # Getting data for one hot encoding\n",
        "  demo_cols = ['METRO', 'AGECAT', 'SEX', 'RACE', 'PHARMA', 'CASETYPE', 'ROUTE_1', 'TOXTEST_1', 'ROUTE_2', 'TOXTEST_2', 'ROUTE_3', 'TOXTEST_3']\n",
        "  case_train_one_hot = X_train[demo_cols]\n",
        "  case_test_one_hot = X_test[demo_cols]\n",
        "\n",
        "  # Getting drug data for category hash encoding\n",
        "  drug_cols = ['DRUGID_1', 'CATID_1_1', 'CATID_2_1', 'CATID_3_1', 'sdled_1_1', 'sdled_2_1', 'sdled_3_1', 'sdled_4_1',\n",
        "        'DRUGID_2', 'CATID_1_2', 'CATID_2_2', 'CATID_3_2', 'sdled_1_2', 'sdled_2_2', 'sdled_3_2', 'sdled_4_2',\n",
        "        'DRUGID_3', 'CATID_1_3', 'CATID_2_3', 'CATID_3_3', 'sdled_1_3', 'sdled_2_3', 'sdled_3_3', 'sdled_4_3']\n",
        "  case_train_drug = X_train[drug_cols]\n",
        "  case_test_drug = X_test[drug_cols]\n",
        "\n",
        "\n",
        "  def oneHotEncode(df):\n",
        "    onehotencode = pd.DataFrame()\n",
        "      \n",
        "    # Onehot code\n",
        "    for feature in df.columns:  \n",
        "      demo_encoded = pd.get_dummies(df[feature], prefix = feature)\n",
        "      onehotencode = pd.concat([onehotencode, demo_encoded], axis = 1)\n",
        "\n",
        "    return onehotencode\n",
        "\n",
        "\n",
        "  def convertToCategory(drug_df):\n",
        "    for c in drug_df.columns:\n",
        "      drug_df[c] = drug_df[c].astype('category')\n",
        "\n",
        "    return drug_df\n",
        "\n",
        "\n",
        "  def hashEncode(category_case_train_drug, c ategory_case_test_drug):\n",
        "    drug_trainhashencoding = pd.DataFrame()\n",
        "    drug_testhashencoding = pd.DataFrame()\n",
        "\n",
        "    # hash encode feature wise\n",
        "    for feature in category_case_train_drug.columns:\n",
        "      # Hashing encoder\n",
        "      encoder = ce.HashingEncoder(cols = feature)\n",
        "      encoder.fit(category_case_train_drug[feature])\n",
        "\n",
        "      # Transform train and test set with hashing encoder object\n",
        "      case_train_drug_hash = encoder.transform(category_case_train_drug)\n",
        "      case_test_drug_hash = encoder.transform(category_case_test_drug)\n",
        "\n",
        "      # Combine\n",
        "      drug_trainhashencoding = pd.concat([case_train_drug_hash, drug_trainhashencoding], axis = 1)\n",
        "      drug_testhashencoding = pd.concat([case_test_drug_hash, drug_testhashencoding], axis = 1)\n",
        "\n",
        "    return drug_trainhashencoding,drug_testhashencoding\n",
        "\n",
        "\n",
        "  # Columns that won't be encoded\n",
        "  data_to_concatenate_last_train = y_train[['ALLABUSE','NONALCILL','ALCOHOL','NONMEDPHARMA','PHARMA']]\n",
        "  data_to_concatenate_last_test = y_test[['ALLABUSE','NONALCILL','ALCOHOL','NONMEDPHARMA', 'PHARMA']]\n",
        "\n",
        "   # Taking out caswgt\n",
        "  casewt_toappend_train = X_train[['CASEWGT']]\n",
        "  casewt_toappend_test = X_test[['CASEWGT']]\n",
        "\n",
        "  # One hot encode demographic columns\n",
        "  case_train_onehot = oneHotEncode(case_train_one_hot).reset_index(drop = True)\n",
        "  case_test_onehot = oneHotEncode(case_test_one_hot).reset_index(drop = True)\n",
        "\n",
        "  # Convert datatype to category\n",
        "  category_case_train_drug = convertToCategory(case_train_drug)\n",
        "  category_case_test_drug = convertToCategory(case_test_drug)\n",
        "\n",
        "  # Hashencode drug related columns\n",
        "  hashencode_case_train, hashencode_case_test = hashEncode(category_case_train_drug, category_case_test_drug)\n",
        "  hashencode_case_train.reset_index(drop = True, inplace = True)\n",
        "  hashencode_case_test.reset_index(drop = True, inplace = True)\n",
        "\n",
        "  # Merge all encodings\n",
        "  final_abuse_train = pd.concat([case_train_onehot, hashencode_case_train, casewt_toappend_train, data_to_concatenate_last_train], axis = 1)\n",
        "  final_abuse_test = pd.concat([case_test_onehot, hashencode_case_train, casewt_toappend_train, data_to_concatenate_last_test], axis = 1)\n",
        "\n",
        "  # Save dataframe\n",
        "  final_abuse_train.to_csv('/content/drive/Shareddrives/CMPE255/data/encoded data/encodedAbuseData_train.csv')\n",
        "  final_abuse_test.to_csv('/content/drive/Shareddrives/CMPE255/data/encoded data/encodedAbuseData_test.csv')\n",
        "\n",
        "  return final_abuse_train, final_abuse_test"
      ],
      "metadata": {
        "id": "ubKF73wHgHTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the encoding, the number of feature columns increases drastically. Thus, reducing the dimensionality becomes an inevident step in the processing pipeline. The dimensionality of the datasets is reducing using PCA and MCA.\n",
        "*   For dataset A, the encodings are then combined and provided to Principal Component Analysis. PCA finds the principal components that explain 80% variance in the data. The resultant dataset has reduced dimensions with minimal information loss which is then input to the model for multilabel classification of the type of abuse. \n",
        "*   For dataset C, the encoded data is passed through MCA (Multiple Correpsondence Analysis) which is similar to PCA but is specific to categorical data. This too reduces the dimensionality of the dataset.\n"
      ],
      "metadata": {
        "id": "o5HrOp9uXIr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pcaAbuse(train, test):\n",
        "\n",
        "  cols = ['ALLABUSE', 'NONALCILL', 'ALCOHOL', 'NONMEDPHARMA', 'PHARMA']\n",
        "\n",
        "  # Separate X and y\n",
        "  X_train = train.drop(cols, axis = 1)\n",
        "  y_train = train[cols]\n",
        "  X_test = test.drop(cols, axis = 1)\n",
        "  y_test = test[cols]\n",
        "\n",
        "  # PCA\n",
        "  pca = PCA(n_components = 0.8).fit(X_train)\n",
        "  X_train_pca = pca.transform(X_train)\n",
        "  X_test_pca = pca.transform(X_test)\n",
        "\n",
        "  # Concatenate X and y columns for both train and test set\n",
        "  X_train_pca_df = pd.DataFrame(data = X_train_pca)\n",
        "  X_train_pca_df.reset_index(drop = True, inplace = True)\n",
        "  y_train.reset_index(drop = True, inplace = True)\n",
        "  X_train_pca = pd.concat([X_train_pca_df, y_train], axis = 1)\n",
        "\n",
        "  X_test_pca_df = pd.DataFrame(data = X_test_pca)\n",
        "  X_test_pca_df.reset_index(drop = True, inplace = True)\n",
        "  y_test.reset_index(drop = True, inplace = True)\n",
        "  X_test_pca = pd.concat([X_test_pca_df, y_test], axis = 1)\n",
        "\n",
        "  # Dump pca object\n",
        "  pickle.dump(pca, open('pca_abuse_obj.pkl', 'wb'))\n",
        "\n",
        "  # Save df\n",
        "  X_train_pca.to_csv('X_train_abuse.csv')\n",
        "  X_test_pca.to_csv('X_test_abuse.csv')   "
      ],
      "metadata": {
        "id": "bPhVhINbJgWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mcaCasetype(train, test):\n",
        "  train_case.drop(['Unnamed: 0', 'CASEWGT'], axis = 1, inplace = True)\n",
        "  test_case.drop(['Unnamed: 0', 'CASEWGT'], axis = 1, inplace = True)\n",
        "\n",
        "  train_cols = ['CASETYPE_1', 'CASETYPE_2', 'CASETYPE_3', 'CASETYPE_4', 'CASETYPE_5', 'CASETYPE_6', 'CASETYPE_7']\n",
        "  test_cols = ['CASETYPE_8']\n",
        "\n",
        "  # Separate X and y\n",
        "  X_train = train.drop(train_cols, axis = 1)\n",
        "  y_train = traintrain_cols]\n",
        "  X_test = test.drop(test_cols, axis = 1)\n",
        "  y_test = test[test_cols]\n",
        "\n",
        "  # Map value for MCA\n",
        "  X_train.replace({0: 'False', 1: 'True'}, inplace = True)\n",
        "  X_test.replace({0: 'False', 1: 'True'}, inplace = True)\n",
        "\n",
        "  # MCA\n",
        "  mca = prince.MCA().fit(X_train)\n",
        "  X_train_mca = mca.transform(X_train)\n",
        "  X_test_mca = mca.transform(X_test)\n",
        "\n",
        "  # Concatenate X and y columns for both train and test set\n",
        "  X_train_mca_df = pd.DataFrame(data = X_train_pca)\n",
        "  X_train_mca_df.reset_index(drop = True, inplace = True)\n",
        "  y_train.reset_index(drop = True, inplace = True)\n",
        "  X_train_mca = pd.concat([X_train_mca_df, y_train], axis = 1)\n",
        "\n",
        "  X_test_mca_df = pd.DataFrame(data = X_test_pca)\n",
        "  X_test_mca_df.reset_index(drop = True, inplace = True)\n",
        "  y_test.reset_index(drop = True, inplace = True)\n",
        "  X_test_mca = pd.concat([X_test_mca_df, y_test], axis = 1)\n",
        "\n",
        "  # Save dataframes\n",
        "  X_train_mca.to_csv('/content/drive/Shareddrives/CMPE255/data/pca/X_train_mca_casetype.csv')\n",
        "  X_test_mca.to_csv('/content/drive/Shareddrives/CMPE255/data/pca/X_test_mca_casetype.csv')"
      ],
      "metadata": {
        "id": "bHzizaXQ5BoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function calls for data preparation\n",
        "\n",
        "demographic_info_df, case_info_df, drug_info_df = getRawData()\n",
        "case_info_df, reduced_drug_info_df = removeOutliers(case_info_df, drug_info_df)\n",
        "final_df = cleanData(reduced_drug_info_df, case_info_df, demographic_info_df)\n",
        "createMappingsDf(final_df)\n",
        "\n",
        "encoded_abuse_train_df, encoded_abuse_test_df = encodingCasetype(final_df)\n",
        "encoded_casetype_train_df, encoded_casetype_test_df = encodingAbuse(final_df)\n",
        "\n",
        "pcaAbuse(encoded_abuse_train_df, encoded_abuse_test_df\n",
        "\n",
        "mcaCasetype(encoded_casetype_train_df, encoded_casetype_test_df)"
      ],
      "metadata": {
        "id": "hT_pgIjzJg55"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}